{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 12 - Advanced Spark\n",
    "\n",
    "## Learning tasks\n",
    "* Read through the online lesson notes and work through Jupyter Notebook version.\n",
    "* Complete the non-graded exercises at the end of the lesson.\n",
    "* Contribute to the discussion for the lesson. Discussion forum 12 will be closed by 11:59 p.m. Day 7 of Week 12. For further details consult the course syllabus.\n",
    "* Submit Assignment 3 no later than 11:59 p.m. Day 7 of Lesson 12. For further details consult the course syllabus.\n",
    "* Work on your group Course project due at the end of the course.\n",
    "\n",
    "## Lesson objectives\n",
    "\n",
    "* Use topic modelling to generate topics for a set of documents, and then compute their contribution to each document\n",
    "* Explain the difference between a Resilient Distributed Dataset (RDD) and a DataFrame\n",
    "* Use Spark to process multiple binary files\n",
    "* Use Optical Character Recognition (OCR) software to large sets of scans of documents in Spark \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of second Spark lesson is to present more advanced features, and to show how it can be used in a general way to process and analyze large numbers of files. Spark includes many built in libraries, parallelized and easy to use. In particular, there is the machine learning (ML) library. We will use it to do topic modelling on the set of books that we have prepared in the previous lesson. Topic modelling allows us to determine a set of topics which the documents are about, and then determine which topic are contained in each document.  \n",
    "\n",
    "## Topic modelling\n",
    "Topic modelling is a purely statistical technique which treats documents as bags (sets) of words, with each word given a unique identifying number. Only the frequency of each word in the document is considered, as the algorithm knows nothing about the language or meaning of words.  Topic modelling then attempts to find sets of words, or topics, which tend to appear in subsets of documents more frequently.  The power of this approach is that the topics themselves don't have to be defined by a human a priori, but instead are determined by the algorithm itself.  In other words, a human does not need to spend time reading a large number of documents to determine what the topics are.  Once the topics are determined, then the presence of the topics in each document can be calculated.  The topics themselves are also just sets of words to the algorithm, and it is up to the human to look at the words most frequenly appearing to determine the meaning of the topic.\n",
    "\n",
    "To make this idea even clearer, let's consider a simple example.  Imagine you take the set of all emails that you ever received, and want to divide them by topics that they discuss, to make it easier to organize them.  Some of these emails might be about your hobbies, some might be about work.  It is likely that in the hobby related emails a certain set of words will be regularly repeated, and in the emails about work a certain other set of words is repeated.  Topic modelling could discover these repeating sets of words and make a topic out of each set. However topic modelling might discover other topics which fit the data better, and which you would not even have considered if you had to pick a topic list yourself.  The end result is that you obtain a useful classification of your emails without having to read them all, and without having to decide which topics they should be classified by.\n",
    "\n",
    "We are now going to do some topic modelling using the tools provided in Spark. First, to continue where we left off in the last lesson, we read the RDD with set of words saved at the end of last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is possible to control the configuration of spark at runtime\n",
    "#import pyspark\n",
    "#config = pyspark.SparkConf().setAll([('spark.executor.memory', '8g'), ('spark.executor.cores', '3'), ('spark.cores.max', '3'), ('spark.driver.memory','8g')])\n",
    "#config = pyspark.SparkConf().setAll([ ('spark.executor.extraJavaOptions', 'Xmx1024m') ,])\n",
    "\n",
    "#sc.stop()\n",
    "#sc = pyspark.SparkContext(conf=config)\n",
    "\n",
    "# NOTE: this loads a restricted data set generated for files in range [A-F]\n",
    "# this is needed to keep the workload reasonable (and to avoid spark crashes)\n",
    "modernism_word_filt_string=sc.textFile(\"/cp631/lesson12/modernism_word_filt/part-*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remind ourselves what this RDD contains, we print the first 10 elements.  We see it contains strings, each string with Python syntax for a tuple containing the document identifier (URL) and then a word in that document.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_filt_string.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert that string to a Python data structure. We could use json module however that has problems with interpreting double quotes.  Instead, will use the ast module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "modernism_word_filt=modernism_word_filt_string.map(lambda x:ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is in the form of a tuple containing two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_filt.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we read in the second RDD prepared in the previous lesson, the one containing word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_count_string=sc.textFile(\"/cp631/lesson12/modernism_word_count/part-*\")\n",
    "modernism_word_count_string.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we convert the string to a Python tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_count=modernism_word_count_string.map(lambda x:ast.literal_eval(x))\n",
    "modernism_word_count.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check how many words are in our RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_count.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next filter out words which occur too frequently, and also those which appear too infrequently.  Words which occur in all documents are not useful to us, since the whole goal is to find distict words that can be used to describe a topic.  We have already filtered out the common stopwords in the previous lesson when preparing this RDD, so here we are just proceeding further with the same idea.  Similarly, very rare words are not going to be useful in determining topics, and they will also increase the time it takes to do the calculation, so removing them makes sense as well.\n",
    "\n",
    "The filter thresholds are somewhat arbitrary, and they could be varied to see if a better topic set can be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_word_count_truncated=modernism_word_count.filter(lambda x: int(x[1])<10000 and int(x[1])>20)\n",
    "# heavily restrict words for debugging\n",
    "#modernism_word_count_truncated=modernism_word_count.filter(lambda x: int(x[1])<600 and int(x[1])>300)\n",
    "\n",
    "modernism_word_count_truncated.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the filter we are left with a smaller number of words to consider.\n",
    "\n",
    "Now we will read in the RDD with the text of selected books generated at the end of previous lesson. That consists of a tuple, containing the URL and another tuple.  That second tuple consists of a dictionary with metadata and then the text of the book.  We again use the ast module to convert a string into an actual Python data structure, then print out some of the data to see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_meta_text_string=sc.textFile(\"/cp631/lesson12/modernism_meta_text/part-*\")\n",
    "import ast\n",
    "modernism_meta_text=modernism_meta_text_string.map(lambda x:ast.literal_eval(x))\n",
    "modernism_meta_text_first=modernism_meta_text.first()\n",
    "print(modernism_meta_text_first[0])\n",
    "print(modernism_meta_text_first[1][0])\n",
    "print(modernism_meta_text_first[1][1][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modelling with LDA\n",
    "\n",
    "The mathematics of topic modelling are quite challenging, even though the idea itself is fairly intuitive, so we will not cover them here.  Spark provides topic modelling techniques via machine learning (ML) libraries. The particular topic modelling technique we will use is based on Latent Dirichlet allocation or LDA.  We next proceed to convert our input RDDs into the mathematical form that the LDA routines will accept.\n",
    "\n",
    "As a first step, we take our list containing the tuple of (word,frequency), and extract only the word, using the keys() method. Then we apply zipWithIndex, which zips the RDD with its element indices.   Quick example to illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, collectAsMap will return the key-value pairs in this RDD to the master as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_vocab = modernism_word_count_truncated.keys().zipWithIndex().collectAsMap()\n",
    "#print(modern_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We broadcast this structure from the master so it is available to all Spark processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "br_modern_vocab = sc.broadcast(modern_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create an RDD containing tuples, each with the url identifier of a book, and a list containing all of its words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_doc_bag = modernism_meta_text.values().map(lambda x: (x[0]['url'], x[1].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We count the number of books in our RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modernism_doc_bag.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform the mathematical transformation.  It is quite involved, so you are welcome to decompose it into stages and print out the results at each stage.  Here we describe the steps.\n",
    "\n",
    "1. Apply filter to keep only the words present in our restricted vocabulary.\n",
    "2. Convert each word to its index, using the dictionary we broadcast, with the word itself as the identifier\n",
    "3. Counter is a tool which takes a list as input, and produces a dictionary containing list members as keys and the number of times each key occurs in the list as values.  Quick example to illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([\"a\",\"a\",\"b\",\"b\",\"c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the Counter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We sort the items in each dictionary, and return an ordered dictionary object, which remembers the order keys were inserted in.\n",
    "5. We convert the data in the dictionary into sparse vector, using one of the methods of the Vectors submodule.  To create a sparse vector, we provide a list of nonzero entries, their indices and their values.\n",
    "6. We perform zipWithIndex, taking the RDD and producing a new one, with pairs consisting of the index and the contents of the original RDD.  Then we map to produce the final RDD by forming a tuple containing the index and the sparse vector.\n",
    "7. We cache the RDD as the final step.\n",
    "\n",
    "We have just outlined a complex multistep process, shown in full below.  The best way to understand it is to run it multiple times, adding the successive stages, and looking at the first element of the resulting RDD each time, to see what each operation does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "mdwc_idx = modernism_doc_bag.mapValues(lambda words: list(filter(lambda word: word in br_modern_vocab.value, words)))\\\n",
    "                  .mapValues(lambda words: list(map(lambda word: br_modern_vocab.value[word], words)))\\\n",
    "                  .mapValues(Counter)\\\n",
    "                  .mapValues(lambda d: OrderedDict(sorted(d.items())))\\\n",
    "                  .mapValues(lambda counter: Vectors.sparse(len(br_modern_vocab.value), list(counter.keys()), list(counter.values())))\\\n",
    "                  .zipWithIndex().map(lambda x: [x[1], x[0][1]])\\\n",
    "                  .cache()\n",
    "\n",
    "# mdwc_idx.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ML (Machine Learning) implementations in Spark\n",
    "- spark.mllib\n",
    "- spark.ml\n",
    " \n",
    "\n",
    "spark.mllib contains the original API build on top of RDDs. We will use spark.ml which provides a higher level API build on top of DataFrames.  The older spark.mllib is still available but it lacks the latest features.  The newer library uses DataFrames instead of RDDs.  DataFrames are an extension of RDDs, storing not only data by labes for the columns in the data as well.  We are not going to discuss DataFrames in detail in this course.  We are only going to create a DataFrame from an RDD, which is done via a createDataFrame routine, which takes as arguments an RDD and a list of labels for the column in the RDD.  Once we are working with DataFrames, the results will be DataFrames also, and each element in the DataFrame will be a Row object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "testdf=spark.createDataFrame(mdwc_idx, [\"id\",\"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass this DataFrame to the LDA model.  In it, we can specify either EMLDAOptimizer (the default) or OnlineLDAOptimizer. We will use the first of these.  We also specify how many topics we want to have generated.  After the LDA object has been initialized, we run its fit method with the DataFrame containing our data as argument.  This may be a time consuming step for a large dataset.  Please note that the technique uses random numbers in its algorithm, so the result will be somewhat different each time (and topics might be ordered differently), unless the same seed is specified explicitly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTopics = 10\n",
    "lda = LDA(k=numTopics, seed=1, optimizer=\"em\")\n",
    "model = lda.fit(testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model object has various methods, which you can see by running the help routine on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(localModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract the topics.  Coming straight out of the model, they are just sets of numbers, which we need to convert to contain corresponding words for these to be useful.  We can limit the outputs to just the top words in each topic for easy readibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicIndices = model.describeTopics(maxTermsPerTopic = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect these topics to take a look at the raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_topicIndices=topicIndices.collect()\n",
    "#print(type(collected_topicIndices))\n",
    "#print(collected_topicIndices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get this into a readable form, we first need to inverse our (word, index) list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modern_top_vocab_inv = {v:k for k, v in modern_vocab.items()}\n",
    "#type(modern_top_vocab_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this inversed list, we now go through the topic list and print out the words that they contain, as well as their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rowind,terms, termWeights in collected_topicIndices:\n",
    "    print(\"TOPIC:\")\n",
    "    for term, weight in zip(terms, termWeights):\n",
    "        print(modern_top_vocab_inv[term], weight)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are machine generated topics and they will not necessarily be intuitive to a human.\n",
    "\n",
    "Now that we know what the topics are, we want to see which topics are contained in each book.  To do this, we apply the transform method of the model to a DataFrame, in this case the DataFrame we have used to fit the model.  We could have also applied it to different data, if it was sufficiently similar to the data we fit the model to.  The transform operation generates a new DataFrame, which will contain the topicDicstribution column for each book.  These contain the contribution of each topic to the book, and the values add up to 100%.  If the contribution for a given topic is high, that means the book contains the words from the topic in a significant degree, and hence is probably about the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed=model.transform(testdf)\n",
    "model.transform(testdf).select(\"id\",\"topicDistribution\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we could, for example, select a topic and then find the books in which these topics are prominently represented. This last step is left as an exercise for the you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple file RDDs applied to analysing newspaper content via OCR\n",
    "\n",
    "The RDDs we have used so far took data from multiple files and gathered it into one set.  However, it is also possible to have an RDD holding separate text and binary files, where each file is distinct.  For text files, this might be a convenient way to keep data from different files separate.  For binary files, this approach is essential if we want to treat them separately, for example if the files are images and we want to apply some operations to them.\n",
    "In this section we are going to use data from the Library of Congress, which has made available online the contents of hundreds of American newspapers published in the 19th and early 20th century.  Much of this data is old enough to be in the Public Domain, which makes it convenient to use.  The data contains scans of newspaper pages, and also the text of those pages obtained via OCR (optical character recognition) software.\n",
    "\n",
    "The techniques of OCR are continually improving, with advances in Machine Learning algorithms, so it often makes sense to perform scans with more modern software to see if the quality of the OCR data can be improved.  However, doing OCR is time consuming, especially if it has to be done on a large data set.  Here is where the parallelization provided by Spark can be very useful, processing large amounts of data efficiently.\n",
    "\n",
    "In the Python code below, we will read data from multiple editions of a newspaper, count the number of occurrences of selected search terms on the front page, and plot them as a function of time.  We will do this first with OCR text data provided online, and then redo the analysis with our own more modern OCR to compare results.\n",
    "\n",
    "The data we will be working with is available at Chronicling America - Historic American Newspapers:  https://chroniclingamerica.loc.gov/lccn .  From the large number of newspapers, we will randomly select the \"New York Tribune\" (1866-1924) for further analysis.  Picking an issue to work in our example, we can look at January 7, 1918 issue at: https://chroniclingamerica.loc.gov/lccn/sn83030214/1918-01-07/ed-1/.  Page 1 of that issue is at https://chroniclingamerica.loc.gov/lccn/sn83030214/1918-01-07/ed-1/seq-1/ and that page has links to the OCR text of the page, as well as the PDF of the scan of the page, i.e. the data we want to scrape.\n",
    "\n",
    "The web scraping program which obtains the data is shown below.  It is not multi-threaded, since we have to be careful about too many simultaneous access requests to the Library of Congress website.  The program downloads the data for a selected range of dates. As the last step of this program, we convert the scan image from PDF format and JPEG, as that is compatible with the tools we will be using below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web scraping program\n",
    "# do not run it, data is already available in /cp631/lesson12/loc_papers\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "def getpdffile(link,datestring):\n",
    "\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < 3:\n",
    "        try:\n",
    "            print(link)\n",
    "            response = urllib.request.urlopen(link, timeout = 5)\n",
    "            content = response.read()\n",
    "            f = open( datestring+\".pdf\", 'wb' )\n",
    "            f.write( content )\n",
    "            f.close()\n",
    "            os.system('convert -density 300 '+datestring+'.pdf  '+datestring+'.jpg')\n",
    "\n",
    "            break\n",
    "\n",
    "        except urllib.error.URLError as e:\n",
    "            attempts += 1\n",
    "            print(type(e))\n",
    "\n",
    "\n",
    "def gethtmlfile(link,datestring):\n",
    "\n",
    "    attempts = 0\n",
    "\n",
    "    while attempts < 3:\n",
    "        try:\n",
    "            print(link)\n",
    "            response = urllib.request.urlopen(link, timeout = 5)\n",
    "            content = response.read().decode()\n",
    "            f = open( datestring+\".txt\", 'w' )\n",
    "            f.write( content )\n",
    "            f.close()\n",
    "\n",
    "            break\n",
    "\n",
    "        except urllib.error.URLError as e:\n",
    "            attempts += 1\n",
    "            print(type(e))\n",
    "\n",
    "\n",
    "link_base=\"https://chroniclingamerica.loc.gov/lccn/sn83030214/\"\n",
    "\n",
    "\n",
    "date_start=datetime.date(year=1918, month=1, day=1)\n",
    "date_start_ord=date_start.toordinal()\n",
    "\n",
    "date_end=datetime.date(year=1918, month=2, day=2)\n",
    "date_end_ord=date_end.toordinal()\n",
    "\n",
    "\n",
    "for i in range(date_start_ord,date_end_ord+1):\n",
    "    dt=datetime.date.fromordinal(i)\n",
    "    datestring=str(dt.year)+\"-\"+str(dt.month).zfill(2)+\"-\"+str(dt.day).zfill(2)\n",
    "    link1=link_base+datestring+\"/ed-1/seq-1/ocr.txt\"\n",
    "    link2=link_base+datestring+\"/ed-1/seq-1.pdf\"\n",
    "\n",
    "    gethtmlfile(link1,datestring)\n",
    "    getpdffile(link2,datestring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the full scan of the front page, displayed in the notebook using the tools available in the IPython module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as Img\n",
    "Img(filename=\"/cp631/lesson12/loc_papers/example/1918-01-07.jpg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(scan above from Library of Congress, it is Public Domain because it was published over 100 years ago, like other figures in this lesson)\n",
    "\n",
    "That is quite a lot of text, and it is has many more columns than modern newspapers.  To make the work for this example a little bit easier, let's look at a section of the above page, the one containing the first article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as Img\n",
    "Img(filename=\"/cp631/lesson12/loc_papers/example/coal-1918-01-07.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the scan is not perfect. The text rows are not straight and the letters are blurred. Even though this text is still quite easy to read for a human being, we will see that OCR software will not be able to read this text perfectly. We could try to improve things by doing some preprocessing on the image, and for example increasing the contrast before feeding the image into OCR software. However, in this case we will just go straight to the text recognition (OCR) step.\n",
    "\n",
    "Let us now look at the OCR text data of this section provided by the Library of Congress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('/cp631/lesson12/loc_papers/example/1918-01-07.txt')\n",
    "text=f.readlines()\n",
    "f.close()\n",
    "for line in text[22:59]:\n",
    "    print(line,end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The software we will be using for OCR is called Tesseract.  It is one of the most popular software packages in its class, and it is free and open source.  It is available as a standard package in most common Linux distribution.  However, the software bundled with Linux is typically older and thus does not incorporate the latest advances in OCR techniques.  To take advantage of those, we will build the latest available version of Tesseract from source and use that.  We will also download recent language training data.\n",
    "\n",
    "Tesseract can be run from the command line.  We can run such commands inside the notebook.  In this case this command will generate the output.txt file which will contain the results of the scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! TESSDATA_PREFIX=/usr/share/tesseract/4/tessdata tesseract /cp631/lesson12/loc_papers/example/coal-1918-01-07.jpg output\n",
    "! cat output.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this command will be available in the file output.txt  However, we want a way to call Tesseract directly inside a Python script and hence inside a Jupyter notebook.  We can do this via a module called pytesseract which provides a wrapper for this purpose.  We set up pytesseract to use the latest tesseract executable we compiled and we also point it to the location of our language data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if import fails, run (after sourcing your virtual environment file)\n",
    "# pip install pytesseract\n",
    "import pytesseract\n",
    "# it is possible to point pytesseract to use a non-default versions\n",
    "#pytesseract.pytesseract.tesseract_cmd = '/home/ubuntu/local/bin/tesseract'\n",
    "#tessdata_dir_config = '--tessdata-dir \"/home/ubuntu\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the PIL module to open an image inside a Python script and pass it to pytesseract, which performs the OCR and stores the result in the string.  Finally, we print the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "im=Image.open('/cp631/lesson12/loc_papers/example/coal-1918-01-07.jpg')\n",
    "#resultOCR=pytesseract.image_to_string(im,config=tessdata_dir_config)\n",
    "\n",
    "resultOCR=pytesseract.image_to_string(im)\n",
    "\n",
    "\n",
    "print(resultOCR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this result to the OCR text obtained from the Library of Congress.  Modern Tesseract seems to be more accurate for some words, though it also makes some mistakes that the older OCR text does not.\n",
    "\n",
    "Our goal is to process multiple pages using Spark. To do this, we want to gather multiple image files of the scan into an RDD. For simplicity the RDD will contain only one file for now, but we could have loaded more files by using a wildcard in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgfiles=sc.binaryFiles('/cp631/lesson12/loc_papers/example/coal-1918-01-07.jpg')\n",
    "# to load more files, we could do:\n",
    "#imgfiles=sc.binaryFiles('/home/ubuntu/adelaide/loc_papers/*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RDD created in this way consists of tuples (pairs), consisting of a string containing the file name, and then the actual binary content of the file.  We want to be able to run a map operation on this RDD, which will take the binary content of each file as output, pass it to pytesseract, and return the OCR text output for that file as a string.\n",
    "\n",
    "The function which will do this is shown below.  This function takes a pair tuple, converts the second part of the pair containing binary data into image object data, runs pytesseract on that, and returns a tuple with filename and the string containing the OCR text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def process_whole_file_with_spark(pair_tuple):\n",
    "    \n",
    "    filename=pair_tuple[0]\n",
    "    image_data=io.BytesIO(pair_tuple[1])\n",
    "    im=Image.open(image_data)\n",
    "#    textOCR = pytesseract.image_to_string(im,config=tessdata_dir_config)\n",
    "    textOCR = pytesseract.image_to_string(im)\n",
    "    return filename,textOCR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, the tesseract executable must be visible to all Spark process, and this will require the directory in which it is located to be in the path.  The content of the path may be examined by running the command below.  Then the softlink to tesseract software may be created with the ln command in one of those directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $PATH\n",
    "# must put tesseract executable in one of the directories listed in path\n",
    "# ln -s /home/ubuntu/local/bin/tesseract tesseract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run the function on the binary files RDD using a map command, generating a new RDD with the string containing the OCR text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_result = imgfiles.map(process_whole_file_with_spark)\n",
    "result=tesseract_result.collect()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have all the tools we need to analyze multiple binary files.  There is a corresponding command for building an RDD out of multiple text files, resulting in an RDD of pairs, containing the string with the filename, and the string with the file content.\n",
    "\n",
    "Let's now read in the OCR text files for all the newspaper editions we have in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now read more files\n",
    "htmlfiles=sc.wholeTextFiles('/cp631/lesson12/loc_papers/*.txt',minPartitions=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now wish to search for a specific term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_function(s):\n",
    "    searchterms=[\"Canada\",\"Canadian\"]\n",
    "                 \n",
    "    count=0\n",
    "    for t in searchterms:\n",
    "        count = count + s.count(t)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply this function to our text files dataset, obtaining the count of the number of times our search terms occur.  We also convert the filename to just the date of the newspaper edition to which the file corresponds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should make list for search terms\n",
    "#searchterm=\"Canadian\"\n",
    "# in the map below we extract just the date from the full file name\n",
    "# 32-42 range will likely need changing on different file system\n",
    "termcount=htmlfiles.map(lambda x:(x[0][32:42],search_function(x[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new RDD now contains a date string and the number of occurences of search terms.   Next we want to plot this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termcount.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For working with dates Python has the datetime module.  This can take a date in string format and convert it to a datetime object, which we will use further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# need list of tuples\n",
    "# each containing: python date object, counts\n",
    "termcount_pythondates=termcount.map(lambda x: (datetime.date(int(x[0][0:4]),int(x[0][5:7]),int(x[0][8:10])),x[1]))\n",
    "termcount_pythondates.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot these dates we collect the RDD into a list on the Spark master process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termcount_pythondates_local=termcount_pythondates.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(termcount_pythondates_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the data.  For this we use numpy and matplotlib modules.  The datetime object for each date can be converted to ordinal, which is a unique integer for each day.  We can then modify data array elements (initialized to zero) for all the dates at which there were newspaper editions.  We finally plot the figure using the special functionality of matplotlib for plotting dates.  If you want a more detailed description of what we are doing here, please refer to online documentation for matplotlib, numpy and datetime.  The function we define for plotting takes as arguments the start and end date for the range of editions that we wish to plot, with all editions being processed having dates falling within the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.dates import drange\n",
    "from numpy import zeros\n",
    "\n",
    "\n",
    "def plot_date_count(ys,ms,ds,ye,me,de,termcount_pythondates_local):\n",
    "    date1 = datetime.datetime(ys,ms,ds)\n",
    "    date2 = datetime.datetime(ye,me,de+1)\n",
    "    delta = datetime.timedelta(days=1)\n",
    "    dates = drange(date1,date2,delta)\n",
    "    data=zeros(len(dates))\n",
    "\n",
    "# fill in the data\n",
    "\n",
    "    for ele in termcount_pythondates_local:\n",
    "        assert ele[0].toordinal()-date1.toordinal() > -1 \n",
    "        assert ele[0].toordinal()-date1.toordinal() < len(data)\n",
    "        data[ele[0].toordinal()-date1.toordinal()]=ele[1]\n",
    "    \n",
    "    fig, ax = matplotlib.pyplot.subplots()\n",
    "\n",
    "    ax.plot_date(dates,data)\n",
    "\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_date_count(1918,1,1,1918,2,3,termcount_pythondates_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(figure generated by me in notebook)\n",
    "\n",
    "Next we want to repeat this plot with text data obtained through our own run of tesseract.  To do this, we form an RDD from all available page scan files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running tesseract is time consuming, so need to select appropriate number of files in this step if reasonble runtime desired\n",
    "#imgfiles=sc.binaryFiles('/cp631/lesson12/loc_papers/*.jpg')\n",
    "#imgfiles=sc.binaryFiles('/cp631/lesson12/loc_papers/1918-01-07.jpg')\n",
    "imgfiles=sc.binaryFiles('/cp631/lesson12/loc_papers/1918-01-0*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to cache this RDD, so that the time consuming RDD step is performed only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_result = imgfiles.map(process_whole_file_with_spark).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tesseract_result_first=tesseract_result.first()\n",
    "print(tesseract_result_first[0])\n",
    "print(tesseract_result_first[1][0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the search function defined above via a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termcount=tesseract_result.map(lambda x:(x[0][32:42],search_function(x[1]))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termcount.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert dates to datetime object via another map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# need list of tuples\n",
    "# each containing: python date object, counts\n",
    "termcount_pythondates=termcount.map(lambda x: (datetime.date(int(x[0][0:4]),int(x[0][5:7]),int(x[0][8:10])),x[1])).cache()\n",
    "termcount_pythondates.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we collect that data and plot the result using function defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termcount_pythondates_local=termcount_pythondates.collect()\n",
    "plot_date_count(1918,1,1,1918,1,9,termcount_pythondates_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(figure generated by me in notebook)\n",
    "\n",
    "We can now compare this graph generated with our OCR using tesseract to one generated from Library of Congress OCR data.  We can see that our Tesseract has fewer term detections, which indicates that it does worse, at least when searching for our terms.\n",
    "\n",
    "You can see how the Spark code we have written enabled us to process multiple binary files with an external program, and then process the results within Spark.  This method is general and could be extended to any workflow in which multile files are processed with various tools.  This is one of the reasons for popularity of Spark when it comes to massive parallel workflows.  As long as the problem we are working on fits the constraints of the Spark paradigm, with each file processing being independent of the others, then Spark is very well suited to the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we have learned some advanced features of Spark.\n",
    "\n",
    "* Spark comes with a set of powerful mathematical tools built in, parallelized and ready to use.  It contains a Machine Learning suite, which provides topic modelling algorithms.\n",
    "* Topic modelling can be used to classify a set of documents without topics defined a priori.  This is highly efficient for large sets of documents which could not be read in a reasonable time by a human.\n",
    "* DataFrames are an extension of RDDs, storing data as well as labels for that data.\n",
    "* Spark can create RDDs out of multiple files, keeping each file as a distinct object.  This is particularly useful for binary files, such as images or videos.\n",
    "* Spark can run arbitrary programs on these binary files, making it very versatile in performing analysis on large datasets in parallel.\n",
    "\n",
    "In the following lesson we will wrap up the whole course with a short lesson summarizing what we have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion exercises\n",
    " \n",
    "Try these exercises and post your efforts on the discussion board.  See the syllabus for more details.\n",
    "\n",
    "1. In the topic modelling section of lesson 12, experiment by changing the number of topics and modifying the cutoffs for the most frequently and least frequently used words.  Try to obtain the most meaningful topics that you can.  Show the parameters that worked best and the top words of the resulting topics.\n",
    "\n",
    "2. In the newspaper dataset text data we were working on, count how many times the ship Titanic is mentioned in each month between 1910 and 1920. Show the code necessary to do that.\n",
    "\n",
    "3. When working on images, it can be useful to pre-process the images first so that the text recognition software does a better job, by either being more accurate or being faster.  Read the documentation of the image processing features of the Image module, and try to modify the image before doing text analysis.  You can for example decrease the resolution of the image, which should result in faster processing at the cost of decreased accuracy.  Measure the performance of the code and observe how the detection count of selected words changes.\n",
    "\n",
    "To locate Discussion forum 12 click on “Discussions” at the top of your MyLS webpage and select “Discussion Forum 12”. For further details and grading rubric consult the course syllabus.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
