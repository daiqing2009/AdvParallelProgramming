{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling large Datasets\n",
    "In data processing, a great deal of computing involves analysing large amounts of text mixed with numerical data.  This is what Spark is particularly suited for. Sampling is an essential pre-processing for machine leanring for proof of concept\n",
    "\n",
    "## Recbole dataset\n",
    "Recbole is a powerful recommendation system traning and evaluation platform. It has many built-in datasets(https://recbole.io/dataset_list.html), some of which is too large to process on a single computer. I will use spark to preprocess it to shrink its size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!rm url.yaml\n",
    "!wget https://raw.githubusercontent.com/RUCAIBox/RecBole/master/recbole/properties/dataset/url.yaml\n",
    "!pip install pyyaml\n",
    "\n",
    "import yaml\n",
    "\n",
    "# Specify the path to the YAML file\n",
    "file_path = \"url.yaml\"\n",
    "\n",
    "# Open the file and load the YAML contents\n",
    "with open(file_path, \"r\") as file:\n",
    "    dataset_urls = yaml.safe_load(file)\n",
    "   \n",
    "# only print the first 5 lines\n",
    "for key in list(dataset_urls.keys())[:5]:\n",
    "    print(key, \":\", dataset_urls[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the datasets to donwload and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_to_download = ['amazon-digital-music', 'amazon-video-games']\n",
    "# datasets_to_download = ['amazon-cds-vinyl']\n",
    "import os\n",
    "# Path to the folder where the zip file will be extracted\n",
    "input_folder_path = \"input\"\n",
    "\n",
    "# Create input folder if it doesn't exist\n",
    "if not os.path.exists(input_folder_path):\n",
    "    os.makedirs(input_folder_path)\n",
    "    \n",
    "# Path to the folder where processed file will be saved\n",
    "output_folder_path = \"output\"\n",
    "\n",
    "# Create out folder if it doesn't exist\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "\n",
    "def download_upzip(url, dataset_name):\n",
    "    # Download the zip file\n",
    "    response = requests.get(url)\n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "\n",
    "    # Extract the zip file to the specified folder of dataset_name\n",
    "    folder_path = os.path.join(input_folder_path, dataset_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    zip_file.extractall(folder_path)\n",
    "\n",
    "    #TODO: if extracted file is a directory, move all files to the parent directory\n",
    "    # for root, dirs, files in os.walk(folder_path):\n",
    "    #     for file in files:\n",
    "    #         os.rename(os.path.join(root, file), os.path.join(folder_path, file))\n",
    "    #     for dir in dirs:\n",
    "    #         os.rmdir(os.path.join(root, dir))\n",
    "\n",
    "    # Close the zip file\n",
    "    zip_file.close()\n",
    "\n",
    "#  download all dataset from datasets_to_download\n",
    "for dataset in datasets_to_download:\n",
    "    download_upzip(dataset_urls[dataset], dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Amazon Sampling\").getOrCreate()\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, count\n",
    "\n",
    "# read from file into dataframe\n",
    "dfs = {}\n",
    "for dataset in datasets_to_download:\n",
    "    dataset_path = os.path.join(input_folder_path, dataset)\n",
    "    dfs[dataset] = {}\n",
    "    for file in os.listdir(dataset_path):\n",
    "        file_path = os.path.join(dataset_path, file)\n",
    "        df = spark.read.option(\"delimiter\",'\\t').option(\"header\", True).csv(file_path)\n",
    "        dfs[dataset][file] = df\n",
    "        print(f\"Dataset: {dataset}, File: {file}\")\n",
    "        df.show(5)\n",
    "        \n",
    "        print(f'num of {file}:',df.count())\n",
    "\n",
    "        # check the uniqueness of key, we assume key name is ending with _id bofore :token i.e. item_id:token\n",
    "        # find the header ending with _id:token\n",
    "        key_columns = [col for col in df.columns if col.endswith('_id:token')]\n",
    "        for key_column in key_columns:\n",
    "            print(f\"Number of disintict {key_column}:\", df.select(key_column).distinct().count())\n",
    "            \n",
    "\n",
    "        # check the completeness of each column\n",
    "        print(\"Number of non-null values in each column:\")\n",
    "        df.select([count(when(col(c).isNotNull() , c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_map = {}\n",
    "# analyze the sparse of the dataset\n",
    "for dataset in datasets_to_download:\n",
    "    dataset_path = os.path.join(input_folder_path, dataset)\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith('.inter'):\n",
    "            inter_map[dataset] = file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter out inactive user/items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inter_threshold = 10\n",
    "item_inter_threshold = 10\n",
    "\n",
    "# filter out the user and item with less than threshold interactions\n",
    "for dataset in datasets_to_download:\n",
    "    print('-----------------------------------')\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    inter_df = dfs[dataset][inter_map[dataset]]\n",
    "    \n",
    "    print(f'num of iteractions:',inter_df.count())\n",
    "\n",
    "    # print(f'num of {inter_map[dataset]}:',inter_df.count())\n",
    "    print(f'num of user_id:',inter_df.select('user_id:token').distinct().count())\n",
    "    print(f'num of item_id:',inter_df.select('item_id:token').distinct().count())\n",
    "    # count the number of interactions for each user and item and rename the count column\n",
    "    user_count_df = inter_df.groupBy('user_id:token').count().withColumnRenamed('count','count_user')\n",
    "    item_count_df = inter_df.groupBy('item_id:token').count().withColumnRenamed('count','count_item')\n",
    "\n",
    "    # append the count of user and item to the original df\n",
    "    inter_df = inter_df.join(user_count_df, on='user_id:token', how='inner')\n",
    "    inter_df = inter_df.join(item_count_df, on='item_id:token', how='inner')\n",
    "    inter_df.show(5)\n",
    "    \n",
    "    # filter out the user and item with less than threshold interactions\n",
    "    inter_df = inter_df.filter((col('count_user') >= user_inter_threshold) & (col('count_item') >= item_inter_threshold))\n",
    "    \n",
    "    print(f'filtered num of iteractions:',inter_df.count())\n",
    "    \n",
    "    # release the memory of dfs[dataset][inter_map[dataset]]\n",
    "    dfs[dataset][inter_map[dataset]] = inter_df.drop('count_user','count_item')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output overlaped users between datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder list of output folders\n",
    "output_folder_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset = datasets_to_download[0]\n",
    "# find the common users between base_dataset and other datasets\n",
    "for j in range(1,len(datasets_to_download)):\n",
    "        dataset1 = base_dataset\n",
    "        dataset2 = datasets_to_download[j]\n",
    "        inter1 = dfs[dataset1][inter_map[dataset1]]\n",
    "        inter2 = dfs[dataset2][inter_map[dataset2]]\n",
    "        inter1.createOrReplaceTempView(\"inter1\")\n",
    "        inter2.createOrReplaceTempView(\"inter2\")\n",
    "\n",
    "        print(f\"Common users between {dataset1} and {dataset2}\")    \n",
    "        # get the distinct users and then intersect\n",
    "        inter1_dist = inter1.select('user_id:token').distinct()\n",
    "        # inter1_dist.show(5)\n",
    "        common_users = inter1_dist.join(inter2, inter1_dist['user_id:token'] == inter2['user_id:token'],'leftsemi')\n",
    "        common_users.show(5)\n",
    "\n",
    "        print(f'num of common_users:',common_users.count())\n",
    "        # print the items count of each inter of common users\n",
    "        inter1_com_user = inter1.join(common_users, 'user_id:token')\n",
    "        inter2_com_user = inter2.join(common_users, 'user_id:token')\n",
    "        # statictics of inter 1\n",
    "        inter1_com_user_count = inter1_com_user.count()\n",
    "        inter1_com_item_count = inter1_com_user.select('item_id:token').distinct().count()\n",
    "        print(f'num of interactino of common users in {dataset1}:',inter1_com_user_count)\n",
    "        print(f'num of related items in the interaction:',inter1_com_item_count)\n",
    "        print(f'density of {dataset1} inetraction :',inter1.count()/inter1_com_user_count/inter1_com_item_count)\n",
    "\n",
    "        # save filtered datasets to file\n",
    "        inter1_out_path = os.path.join(output_folder_path, f\"{dataset1}_{dataset2}\")\n",
    "        inter1_com_user.show(5)\n",
    "        inter1_com_user.repartition(1).write.option(\"header\", \"true\").csv(inter1_out_path, mode='overwrite', sep='\\t')\n",
    "        output_folder_list.append(inter1_out_path)\n",
    "        # output_folder_map[dataset1] = inter1_out_path\n",
    "        \n",
    "        # statictics of inter 2\n",
    "        inter2_com_user_count = inter2_com_user.count()\n",
    "        inter2_com_item_count = inter2_com_user.select('item_id:token').distinct().count()\n",
    "        print(f'num of interactino of common users in {dataset2}:',inter2_com_user_count)\n",
    "        print(f'num of related items in the interaction:',inter2_com_item_count)\n",
    "        print(f'density of {dataset2} inetraction :',inter2.count()/inter2_com_user_count/inter2_com_item_count)\n",
    "\n",
    "        # save filtered datasets to file\n",
    "        inter2_out_path = os.path.join(output_folder_path, f\"{dataset2}_{dataset1}\")\n",
    "        inter2_com_user.show(5) \n",
    "        inter2_com_user.repartition(1).write.option(\"header\", \"true\").csv(inter2_out_path, mode='overwrite', sep='\\t')\n",
    "        output_folder_list.append(inter2_out_path)\n",
    "        # output_folder_map[dataset2] = inter2_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the common users among all downloaded datasets\n",
    "for dataset in datasets_to_download:\n",
    "    inter = dfs[dataset][inter_map[dataset]]\n",
    "    inter.createOrReplaceTempView(\"inter\")\n",
    "\n",
    "    print(f\"Common users among all datasets\")    \n",
    "    # get the distinct users and then intersect\n",
    "    inter_dist = inter.select('user_id:token').distinct()\n",
    "    inter_dist.show(3)\n",
    "    if dataset == datasets_to_download[0]:\n",
    "        common_users = inter_dist\n",
    "    else:\n",
    "        common_users = common_users.join(inter_dist, 'user_id:token','inner')\n",
    "    print(f'num of common_users after merge with {dataset}:',common_users.count())\n",
    "\n",
    "common_users.show(3)\n",
    "\n",
    "# export inter of common users to file\n",
    "for dataset in datasets_to_download:\n",
    "    inter = dfs[dataset][inter_map[dataset]]\n",
    "    inter.createOrReplaceTempView(\"inter\")\n",
    "    inter_com_user = inter.join(common_users, 'user_id:token')\n",
    "    inter_com_user_count = inter_com_user.count()\n",
    "    inter_com_item_count = inter_com_user.select('item_id:token').distinct().count()\n",
    "    print(f'num of interactino of common users in {dataset}:',inter_com_user_count)\n",
    "    print(f'num of {dataset} :',inter_com_item_count)\n",
    "    print(f'density of {dataset} inetraction :',inter.count()/inter_com_user_count/inter_com_item_count)\n",
    "\n",
    "    # save filtered datasets to file\n",
    "    inter_out_path = os.path.join(output_folder_path, f\"{dataset}_common\")\n",
    "    inter_com_user.show(5)\n",
    "    inter_com_user.repartition(1).write.option(\"header\", \"true\").csv(inter_out_path, mode='overwrite', sep='\\t')\n",
    "    output_folder_list.append(inter_out_path)\n",
    "    # output_folder_map[dataset] = inter_out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_itemfile_map = {}\n",
    "def get_itemfile_path(dataset):\n",
    "    dataset_path = os.path.join(input_folder_path, dataset)\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith('.item'):\n",
    "            return os.path.join(dataset_path, file)\n",
    "    return None\n",
    "\n",
    "for ouptput_folder in output_folder_list:\n",
    "    # strip the dataset from the first part of folder\n",
    "    dataset = os.path.basename(ouptput_folder).split('_')[0]\n",
    "    # copy .item file from correonding input folder to output folder\n",
    "    itemfile_path = get_itemfile_path(dataset)\n",
    "    if itemfile_path:\n",
    "        print(f\"copy from {itemfile_path} to {ouptput_folder} for {dataset} \")\n",
    "        out_path = os.path.join(ouptput_folder, f\"{dataset}.item\")\n",
    "        !cp $itemfile_path $out_path\n",
    "    else:\n",
    "        print(f\"item file not found for {dataset}\")\n",
    "\n",
    "    for file in os.listdir(ouptput_folder):\n",
    "        # rename exported cvs as .inter\n",
    "        if file.endswith('.csv'):\n",
    "            # rename file to {folder}.inter\n",
    "            file_path = os.path.join(ouptput_folder, file)\n",
    "            out_path = os.path.join(ouptput_folder, f\"{dataset}.inter\")\n",
    "            !mv $file_path $out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Chronicle Characteristics\n",
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "Stratified sampling based on hotness(interaction rate) of items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## release all the resources "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpersist the dfs\n",
    "for dataset in datasets_to_download:\n",
    "    for key in dfs[dataset]:\n",
    "        dfs[dataset][key].unpersist()\n",
    "        \n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sammary\n",
    "Spark is a powerful and efficient tool to handle sample on large scale of data. \n",
    "* flexible and powerful functionality\n",
    "* runs super fast even on my laptop\n",
    "* easy to apply to similar datasets(Amazon have dataset of different categories), I only focused on one categoy this time. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
